{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from albumentations import RandomCrop, HorizontalFlip, VerticalFlip\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, Callback\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13643527472410155265\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16645043813180727877\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10990968161574586030\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11764238912\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3631244557671988046\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:03:00.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works on HPC. [DO NOT RUN ANYMORE, ALREADY DONE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original images:  400 - Original masks: 400\n"
     ]
    }
   ],
   "source": [
    "## Data Augmentation\n",
    "## It was chosen the resolution of 1536x1024px keep the ratio of the original images (6000x4000px). \n",
    "\n",
    "    \n",
    "def augment_data(images, masks, save_path, augment=True):\n",
    "    H = 1024\n",
    "    W = 1536\n",
    "    for x,y in tqdm(zip(images, masks), total=len(images)):\n",
    "        name = x.split(\"/\")[-1].split(\".\")\n",
    "        image_name = name[0]\n",
    "        image_extn = name[1]\n",
    "\n",
    "        name = y.split(\"/\")[-1].split(\".\")\n",
    "        mask_name = name[0]\n",
    "        mask_extn = name[1]       \n",
    "        \n",
    "        x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        x = cv2.resize(x, (W, H))\n",
    "        y = cv2.imread(y, cv2.IMREAD_COLOR)\n",
    "        y = cv2.resize(y, (W, H))\n",
    "        \n",
    "        if augment == True:\n",
    "            \n",
    "            aug = RandomCrop(int(2*H/3), int(2*W/3), always_apply=False, p=1.0)\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x1 = augmented[\"image\"]\n",
    "            y1 = augmented[\"mask\"]\n",
    " \n",
    "            aug = HorizontalFlip(always_apply=False, p=1.0)\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x2 = augmented[\"image\"]\n",
    "            y2 = augmented[\"mask\"]\n",
    "            \n",
    "            aug = VerticalFlip(always_apply=False, p=1.0)\n",
    "            augmented = aug(image=x, mask=y)\n",
    "            x3 = augmented[\"image\"]\n",
    "            y3 = augmented[\"mask\"] \n",
    "            \n",
    "            save_images = [x, x1, x2, x3]\n",
    "            save_masks = [y, y1, y2, y3]            \n",
    "          \n",
    "        else:\n",
    "            save_images = [x]\n",
    "            save_masks = [y]\n",
    "        \n",
    "        idx = 0\n",
    "        for i, m in zip(save_images, save_masks):\n",
    "            i = cv2.resize(i, (W, H))\n",
    "            m = cv2.resize(m, (W, H))\n",
    "            \n",
    "            tmp_img_name = f\"{image_name}_{idx}.{image_extn}\"\n",
    "            tmp_msk_name = f\"{mask_name}_{idx}.{mask_extn}\" \n",
    "            \n",
    "            image_path = os.path.join(save_path, \"images\", tmp_img_name)\n",
    "            mask_path = os.path.join(save_path, \"masks\", tmp_msk_name)\n",
    "            \n",
    "            cv2.imwrite(image_path, i)\n",
    "            cv2.imwrite(mask_path, m)\n",
    "\n",
    "            idx+=1\n",
    "\n",
    "path = \"../data/dataset/semantic_drone_dataset\"\n",
    "images = sorted(glob(os.path.join(path, \"original_images/*\")))\n",
    "masks = sorted(glob(os.path.join(path, \"label_images_semantic/*\")))\n",
    "print(f\"Original images:  {len(images)} - Original masks: {len(masks)}\")\n",
    "\n",
    "create_dir(\"../data/dataset/semantic_drone_dataset/new_data/images/\")\n",
    "create_dir(\"../data/dataset/semantic_drone_dataset/new_data/masks/\")\n",
    "\n",
    "save_path = \"../data/dataset/semantic_drone_dataset/new_data/\"\n",
    "\n",
    "augment_data(images, masks, save_path, augment=True)\n",
    "\n",
    "images = sorted(glob(os.path.join(save_path, \"images/*\")))\n",
    "masks = sorted(glob(os.path.join(save_path, \"masks/*\")))\n",
    "print(f\"Augmented images:  {len(images)} - Augmented masks: {len(masks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original images:  400 - Original masks: 400\n",
      "Augmented images:  1600 - Augmented masks: 1600\n"
     ]
    }
   ],
   "source": [
    "path = \"../data/dataset/semantic_drone_dataset\"\n",
    "images = sorted(glob(os.path.join(path, \"original_images/*\")))\n",
    "masks = sorted(glob(os.path.join(path, \"label_images_semantic/*\")))\n",
    "print(f\"Original images:  {len(images)} - Original masks: {len(masks)}\")\n",
    "\n",
    "save_path = \"../data/dataset/semantic_drone_dataset/new_data/\"\n",
    "\n",
    "images = sorted(glob(os.path.join(save_path, \"images/*\")))\n",
    "masks = sorted(glob(os.path.join(save_path, \"masks/*\")))\n",
    "print(f\"Augmented images:  {len(images)} - Augmented masks: {len(masks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images:  1600\n"
     ]
    }
   ],
   "source": [
    "## Create dataframe\n",
    "\n",
    "image_path =  os.path.join(save_path, \"images/\")\n",
    "label_path = os.path.join(save_path, \"masks/\")\n",
    "\n",
    "def create_dataframe(path):\n",
    "    name = []\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            name.append(filename.split('.')[0])\n",
    "    \n",
    "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n",
    "\n",
    "df_images = create_dataframe(image_path)\n",
    "df_masks = create_dataframe(label_path)\n",
    "print('Total Images: ', len(df_images))\n",
    "#print(df_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size : 1280 images\n",
      "Val Size   :  320 images\n",
      "Test Size  :  160 images\n"
     ]
    }
   ],
   "source": [
    "## Split data\n",
    "\n",
    "X_trainval, X_test = train_test_split(df_images['id'], test_size=0.1, random_state=19)\n",
    "X_trainval=df_images['id']\n",
    "X_train, X_val = train_test_split(X_trainval, test_size=0.2, random_state=19)\n",
    "\n",
    "print(f\"Train Size : {len(X_train)} images\")\n",
    "print(f\"Val Size   :  {len(X_val)} images\")\n",
    "print(f\"Test Size  :  {len(X_test)} images\")\n",
    "\n",
    "y_train = X_train #the same values for images (X) and labels (y)\n",
    "y_test = X_test\n",
    "y_val = X_val\n",
    "\n",
    "img_train = [os.path.join(image_path, f\"{name}.jpg\") for name in X_train]\n",
    "mask_train = [os.path.join(label_path, f\"{name}.png\") for name in y_train]\n",
    "img_val = [os.path.join(image_path, f\"{name}.jpg\") for name in X_val]\n",
    "mask_val = [os.path.join(label_path, f\"{name}.png\") for name in y_val]\n",
    "img_test = [os.path.join(image_path, f\"{name}.jpg\") for name in X_test]\n",
    "mask_test = [os.path.join(label_path, f\"{name}.png\") for name in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, SeparableConv2D, BatchNormalization, MaxPooling2D, Conv2DTranspose\n",
    "from tensorflow.keras.layers import concatenate, Conv2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def mobileunet(input_size, num_classes):\n",
    "    inputs = Input(input_size)\n",
    "\n",
    "    conv1  = SeparableConv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1  = BatchNormalization()(conv1)\n",
    "    conv1  = SeparableConv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    conv1  = BatchNormalization()(conv1)\n",
    "    pool1  = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    \n",
    "    conv2  = SeparableConv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2  = BatchNormalization()(conv2)\n",
    "    conv2  = SeparableConv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    conv2  = BatchNormalization()(conv2)\n",
    "    pool2  = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    \n",
    "    conv3  = SeparableConv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3  = BatchNormalization()(conv3)\n",
    "    conv3  = SeparableConv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    conv3  = BatchNormalization()(conv3)\n",
    "    pool3  = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    \n",
    "    conv4  = SeparableConv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4  = BatchNormalization()(conv4)\n",
    "    conv4  = SeparableConv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "    conv4  = BatchNormalization()(conv4)\n",
    "    pool4  = MaxPooling2D(pool_size=(2, 2))(conv4)    \n",
    "    \n",
    "    conv5  = SeparableConv2D(1024, 3, activation='relu', padding='same')(pool4)\n",
    "    conv5  = BatchNormalization()(conv5)\n",
    "    conv5  = SeparableConv2D(1024, 3, activation='relu', padding='same')(conv5)\n",
    "    conv5  = BatchNormalization()(conv5)\n",
    "    \n",
    "    conv6  = Conv2DTranspose(512, 3, strides=(2, 2), activation='relu', padding='same')(conv5)\n",
    "    cat6   = concatenate([conv4, conv6], axis = 3)\n",
    "    conv6  = SeparableConv2D(512, 3, activation='relu', padding='same')(cat6)\n",
    "    conv6  = BatchNormalization()(conv6)\n",
    "    conv6  = SeparableConv2D(512, 3, activation='relu', padding='same')(conv6)\n",
    "    conv6  = BatchNormalization()(conv6)\n",
    "    \n",
    "    conv7  = Conv2DTranspose(256, 3, strides=(2, 2), activation='relu', padding='same')(conv6)\n",
    "    cat7   = concatenate([conv3, conv7], axis = 3)\n",
    "    conv7  = SeparableConv2D(256, 3, activation='relu', padding='same')(cat7)\n",
    "    conv7  = BatchNormalization()(conv7)\n",
    "    conv7  = SeparableConv2D(256, 3, activation='relu', padding='same')(conv7)\n",
    "    conv7  = BatchNormalization()(conv7)\n",
    "    \n",
    "    conv8  = Conv2DTranspose(128, 3, strides=(2, 2), activation='relu', padding='same')(conv7)\n",
    "    cat8   = concatenate([conv2, conv8], axis = 3)\n",
    "    conv8  = SeparableConv2D(128, 3, activation='relu', padding='same')(cat8)\n",
    "    conv8  = BatchNormalization()(conv8)\n",
    "    conv8  = SeparableConv2D(128, 3, activation='relu', padding='same')(conv8)    \n",
    "    conv8  = BatchNormalization()(conv8)\n",
    "    \n",
    "    conv9  = Conv2DTranspose(64, 3, strides=(2, 2), activation='relu', padding='same')(conv8)\n",
    "    cat9   = concatenate([conv1, conv9], axis = 3)\n",
    "    conv9  = SeparableConv2D(64, 3, activation='relu', padding='same')(cat9)\n",
    "    conv9  = BatchNormalization()(conv9)\n",
    "    conv9  = SeparableConv2D(64, 3, activation='relu', padding='same')(conv9)        \n",
    "    conv9  = BatchNormalization()(conv9)\n",
    "    conv9  = Conv2D(2, 3, activation='relu', padding='same')(conv9)\n",
    "    conv10 = Conv2D(num_classes, 1, activation='sigmoid')(conv9)\n",
    "    \n",
    "    output = conv10\n",
    "    return Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the resolution of the images and the number of classes\n",
    "\n",
    "\n",
    "#H = 768   #to keep the original ratio \n",
    "#W = 1152 \n",
    "H = 320   #to keep the original ratio \n",
    "W = 480 \n",
    "\n",
    "num_classes = 23\n",
    "\n",
    "#model = build_unet((W, H, 3), num_classes)  \n",
    "model = mobileunet((W, H, 3),num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 480, 320, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 480, 320, 64) 283         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 480, 320, 64) 256         separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 480, 320, 64) 4736        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 480, 320, 64) 256         separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 240, 160, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 240, 160, 128 8896        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 240, 160, 128 512         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 240, 160, 128 17664       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 240, 160, 128 512         separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 120, 80, 128) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 120, 80, 256) 34176       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 120, 80, 256) 1024        separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 120, 80, 256) 68096       batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 120, 80, 256) 1024        separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 60, 40, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 60, 40, 512)  133888      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 60, 40, 512)  2048        separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 60, 40, 512)  267264      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 60, 40, 512)  2048        separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 30, 20, 512)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 30, 20, 1024) 529920      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 30, 20, 1024) 4096        separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 30, 20, 1024) 1058816     batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 30, 20, 1024) 4096        separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 60, 40, 512)  4719104     batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60, 40, 1024) 0           batch_normalization_7[0][0]      \n",
      "                                                                 conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_10 (SeparableC (None, 60, 40, 512)  534016      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 60, 40, 512)  2048        separable_conv2d_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_11 (SeparableC (None, 60, 40, 512)  267264      batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 60, 40, 512)  2048        separable_conv2d_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 120, 80, 256) 1179904     batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 120, 80, 512) 0           batch_normalization_5[0][0]      \n",
      "                                                                 conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_12 (SeparableC (None, 120, 80, 256) 135936      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 120, 80, 256) 1024        separable_conv2d_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_13 (SeparableC (None, 120, 80, 256) 68096       batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 120, 80, 256) 1024        separable_conv2d_13[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 240, 160, 128 295040      batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 240, 160, 256 0           batch_normalization_3[0][0]      \n",
      "                                                                 conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_14 (SeparableC (None, 240, 160, 128 35200       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 240, 160, 128 512         separable_conv2d_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_15 (SeparableC (None, 240, 160, 128 17664       batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 240, 160, 128 512         separable_conv2d_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 480, 320, 64) 73792       batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 480, 320, 128 0           batch_normalization_1[0][0]      \n",
      "                                                                 conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_16 (SeparableC (None, 480, 320, 64) 9408        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 480, 320, 64) 256         separable_conv2d_16[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_17 (SeparableC (None, 480, 320, 64) 4736        batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 480, 320, 64) 256         separable_conv2d_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 480, 320, 2)  1154        batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 480, 320, 23) 69          conv2d[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 9,488,674\n",
      "Trainable params: 9,476,898\n",
      "Non-trainable params: 11,776\n",
      "__________________________________________________________________________________________________\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "plot_model(model,to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset Pipeline used for training the model\n",
    "\n",
    "def read_image(x):\n",
    "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "\n",
    "def read_mask(x):\n",
    "    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x.astype(np.int32)\n",
    "    return x\n",
    "\n",
    "\n",
    "def tf_dataset(x,y, batch=4):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "    dataset = dataset.shuffle(buffer_size=500)\n",
    "    dataset = dataset.map(preprocess)\n",
    "    dataset = dataset.batch(batch)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(2)\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "def preprocess(x,y):\n",
    "    def f(x,y):\n",
    "        x = x.decode()\n",
    "        y = y.decode()\n",
    "        image = read_image(x)\n",
    "        mask = read_mask(y)\n",
    "        return image, mask\n",
    "    \n",
    "    image, mask = tf.numpy_function(f,[x,y],[tf.float32, tf.int32])\n",
    "    mask = tf.one_hot(mask, num_classes, dtype=tf.int32)\n",
    "    image.set_shape([H, W, 3])    # In the Images, number of channels = 3. \n",
    "    mask.set_shape([H, W, num_classes])    # In the Masks, number of channels = number of classes. \n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 3.1395 - accuracy: 0.2775\n",
      "Epoch 00001: val_loss improved from inf to 3.11941, saving model to ./model.h5\n",
      "320/320 [==============================] - 277s 867ms/step - loss: 3.1395 - accuracy: 0.2775 - val_loss: 3.1194 - val_accuracy: 0.3924 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 3.0982 - accuracy: 0.3774\n",
      "Epoch 00002: val_loss improved from 3.11941 to 3.10335, saving model to ./model.h5\n",
      "320/320 [==============================] - 263s 821ms/step - loss: 3.0982 - accuracy: 0.3774 - val_loss: 3.1034 - val_accuracy: 0.3930 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 3.0709 - accuracy: 0.3876\n",
      "Epoch 00003: val_loss improved from 3.10335 to 3.04724, saving model to ./model.h5\n",
      "320/320 [==============================] - 265s 828ms/step - loss: 3.0709 - accuracy: 0.3876 - val_loss: 3.0472 - val_accuracy: 0.4185 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 3.0382 - accuracy: 0.4151\n",
      "Epoch 00004: val_loss improved from 3.04724 to 3.02653, saving model to ./model.h5\n",
      "320/320 [==============================] - 262s 820ms/step - loss: 3.0382 - accuracy: 0.4151 - val_loss: 3.0265 - val_accuracy: 0.4266 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 3.0026 - accuracy: 0.4224\n",
      "Epoch 00005: val_loss improved from 3.02653 to 2.99952, saving model to ./model.h5\n",
      "320/320 [==============================] - 257s 804ms/step - loss: 3.0026 - accuracy: 0.4224 - val_loss: 2.9995 - val_accuracy: 0.4194 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 2.9631 - accuracy: 0.4262\n",
      "Epoch 00006: val_loss improved from 2.99952 to 2.94294, saving model to ./model.h5\n",
      "320/320 [==============================] - 259s 810ms/step - loss: 2.9631 - accuracy: 0.4262 - val_loss: 2.9429 - val_accuracy: 0.4154 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 2.9178 - accuracy: 0.4278\n",
      "Epoch 00007: val_loss improved from 2.94294 to 2.89793, saving model to ./model.h5\n",
      "320/320 [==============================] - 261s 815ms/step - loss: 2.9178 - accuracy: 0.4278 - val_loss: 2.8979 - val_accuracy: 0.4337 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 2.8704 - accuracy: 0.4221\n",
      "Epoch 00008: val_loss improved from 2.89793 to 2.85677, saving model to ./model.h5\n",
      "320/320 [==============================] - 268s 838ms/step - loss: 2.8704 - accuracy: 0.4221 - val_loss: 2.8568 - val_accuracy: 0.4224 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 2.8107 - accuracy: 0.3021\n",
      "Epoch 00009: val_loss improved from 2.85677 to 2.75817, saving model to ./model.h5\n",
      "320/320 [==============================] - 260s 814ms/step - loss: 2.8107 - accuracy: 0.3021 - val_loss: 2.7582 - val_accuracy: 0.1519 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 2.6391 - accuracy: 0.1329\n",
      "Epoch 00010: val_loss improved from 2.75817 to 2.50755, saving model to ./model.h5\n",
      "320/320 [==============================] - 261s 815ms/step - loss: 2.6391 - accuracy: 0.1329 - val_loss: 2.5076 - val_accuracy: 0.0892 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 2.3659 - accuracy: 0.0992\n",
      "Epoch 00011: val_loss improved from 2.50755 to 2.30378, saving model to ./model.h5\n",
      "320/320 [==============================] - 261s 816ms/step - loss: 2.3659 - accuracy: 0.0992 - val_loss: 2.3038 - val_accuracy: 0.1245 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 2.1073 - accuracy: 0.0977\n",
      "Epoch 00012: val_loss improved from 2.30378 to 2.02393, saving model to ./model.h5\n",
      "320/320 [==============================] - 261s 815ms/step - loss: 2.1073 - accuracy: 0.0977 - val_loss: 2.0239 - val_accuracy: 0.0937 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.9232 - accuracy: 0.0975\n",
      "Epoch 00013: val_loss improved from 2.02393 to 1.85278, saving model to ./model.h5\n",
      "320/320 [==============================] - 261s 814ms/step - loss: 1.9232 - accuracy: 0.0975 - val_loss: 1.8528 - val_accuracy: 0.0987 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.7601 - accuracy: 0.1731\n",
      "Epoch 00014: val_loss improved from 1.85278 to 1.69251, saving model to ./model.h5\n",
      "320/320 [==============================] - 259s 810ms/step - loss: 1.7601 - accuracy: 0.1731 - val_loss: 1.6925 - val_accuracy: 0.2767 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.5886 - accuracy: 0.4721\n",
      "Epoch 00015: val_loss improved from 1.69251 to 1.50777, saving model to ./model.h5\n",
      "320/320 [==============================] - 260s 811ms/step - loss: 1.5886 - accuracy: 0.4721 - val_loss: 1.5078 - val_accuracy: 0.5601 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.4311 - accuracy: 0.5633\n",
      "Epoch 00016: val_loss improved from 1.50777 to 1.44441, saving model to ./model.h5\n",
      "320/320 [==============================] - 265s 828ms/step - loss: 1.4311 - accuracy: 0.5633 - val_loss: 1.4444 - val_accuracy: 0.5548 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.3310 - accuracy: 0.5750\n",
      "Epoch 00017: val_loss improved from 1.44441 to 1.43670, saving model to ./model.h5\n",
      "320/320 [==============================] - 264s 824ms/step - loss: 1.3310 - accuracy: 0.5750 - val_loss: 1.4367 - val_accuracy: 0.5419 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.2782 - accuracy: 0.5854\n",
      "Epoch 00018: val_loss improved from 1.43670 to 1.37047, saving model to ./model.h5\n",
      "320/320 [==============================] - 263s 822ms/step - loss: 1.2782 - accuracy: 0.5854 - val_loss: 1.3705 - val_accuracy: 0.5504 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.2347 - accuracy: 0.5858\n",
      "Epoch 00019: val_loss did not improve from 1.37047\n",
      "320/320 [==============================] - 263s 822ms/step - loss: 1.2347 - accuracy: 0.5858 - val_loss: 1.3849 - val_accuracy: 0.5557 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.2054 - accuracy: 0.5831\n",
      "Epoch 00020: val_loss improved from 1.37047 to 1.33493, saving model to ./model.h5\n",
      "320/320 [==============================] - 262s 820ms/step - loss: 1.2054 - accuracy: 0.5831 - val_loss: 1.3349 - val_accuracy: 0.5568 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1755 - accuracy: 0.5824\n",
      "Epoch 00021: val_loss did not improve from 1.33493\n",
      "320/320 [==============================] - 262s 818ms/step - loss: 1.1755 - accuracy: 0.5824 - val_loss: 1.3626 - val_accuracy: 0.5712 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1495 - accuracy: 0.5855\n",
      "Epoch 00022: val_loss improved from 1.33493 to 1.30268, saving model to ./model.h5\n",
      "320/320 [==============================] - 262s 818ms/step - loss: 1.1495 - accuracy: 0.5855 - val_loss: 1.3027 - val_accuracy: 0.5586 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1357 - accuracy: 0.5835\n",
      "Epoch 00023: val_loss did not improve from 1.30268\n",
      "320/320 [==============================] - 261s 817ms/step - loss: 1.1357 - accuracy: 0.5835 - val_loss: 1.3192 - val_accuracy: 0.5481 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.1192 - accuracy: 0.5818\n",
      "Epoch 00024: val_loss did not improve from 1.30268\n",
      "320/320 [==============================] - 261s 815ms/step - loss: 1.1192 - accuracy: 0.5818 - val_loss: 1.3258 - val_accuracy: 0.5605 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0948 - accuracy: 0.5843\n",
      "Epoch 00025: val_loss did not improve from 1.30268\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "320/320 [==============================] - 262s 818ms/step - loss: 1.0948 - accuracy: 0.5843 - val_loss: 1.3168 - val_accuracy: 0.5634 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0686 - accuracy: 0.5844\n",
      "Epoch 00026: val_loss improved from 1.30268 to 1.27589, saving model to ./model.h5\n",
      "320/320 [==============================] - 262s 820ms/step - loss: 1.0686 - accuracy: 0.5844 - val_loss: 1.2759 - val_accuracy: 0.5546 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0555 - accuracy: 0.5845\n",
      "Epoch 00027: val_loss improved from 1.27589 to 1.27557, saving model to ./model.h5\n",
      "320/320 [==============================] - 262s 817ms/step - loss: 1.0555 - accuracy: 0.5845 - val_loss: 1.2756 - val_accuracy: 0.5552 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0561 - accuracy: 0.5842\n",
      "Epoch 00028: val_loss did not improve from 1.27557\n",
      "320/320 [==============================] - 260s 812ms/step - loss: 1.0561 - accuracy: 0.5842 - val_loss: 1.2854 - val_accuracy: 0.5509 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0509 - accuracy: 0.5846\n",
      "Epoch 00029: val_loss did not improve from 1.27557\n",
      "320/320 [==============================] - 261s 815ms/step - loss: 1.0509 - accuracy: 0.5846 - val_loss: 1.3044 - val_accuracy: 0.5481 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0453 - accuracy: 0.5846\n",
      "Epoch 00030: val_loss did not improve from 1.27557\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1e-06.\n",
      "320/320 [==============================] - 262s 817ms/step - loss: 1.0453 - accuracy: 0.5846 - val_loss: 1.2878 - val_accuracy: 0.5515 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0433 - accuracy: 0.5841\n",
      "Epoch 00031: val_loss did not improve from 1.27557\n",
      "320/320 [==============================] - 262s 818ms/step - loss: 1.0433 - accuracy: 0.5841 - val_loss: 1.2823 - val_accuracy: 0.5527 - lr: 1.0000e-06\n",
      "Epoch 32/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0440 - accuracy: 0.5847\n",
      "Epoch 00032: val_loss did not improve from 1.27557\n",
      "320/320 [==============================] - 262s 817ms/step - loss: 1.0440 - accuracy: 0.5847 - val_loss: 1.2811 - val_accuracy: 0.5530 - lr: 1.0000e-06\n",
      "Epoch 33/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0407 - accuracy: 0.5845\n",
      "Epoch 00033: val_loss did not improve from 1.27557\n",
      "320/320 [==============================] - 261s 816ms/step - loss: 1.0407 - accuracy: 0.5845 - val_loss: 1.2814 - val_accuracy: 0.5527 - lr: 1.0000e-06\n",
      "Epoch 34/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0424 - accuracy: 0.5844\n",
      "Epoch 00034: val_loss did not improve from 1.27557\n",
      "320/320 [==============================] - 261s 817ms/step - loss: 1.0424 - accuracy: 0.5844 - val_loss: 1.2852 - val_accuracy: 0.5515 - lr: 1.0000e-06\n",
      "Epoch 35/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0409 - accuracy: 0.5844\n",
      "Epoch 00035: val_loss did not improve from 1.27557\n",
      "320/320 [==============================] - 262s 818ms/step - loss: 1.0409 - accuracy: 0.5844 - val_loss: 1.2788 - val_accuracy: 0.5542 - lr: 1.0000e-06\n",
      "Epoch 36/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0429 - accuracy: 0.5841\n",
      "Epoch 00036: val_loss did not improve from 1.27557\n",
      "320/320 [==============================] - 262s 818ms/step - loss: 1.0429 - accuracy: 0.5841 - val_loss: 1.2816 - val_accuracy: 0.5525 - lr: 1.0000e-06\n",
      "Epoch 37/50\n",
      "320/320 [==============================] - ETA: 0s - loss: 1.0411 - accuracy: 0.5850"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "\n",
    "# Seeding\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "shape = (H, W, 3)\n",
    "num_classes = 23  \n",
    "lr = 1e-4\n",
    "batch_size = 4\n",
    "epochs = 50\n",
    "\n",
    "# Model\n",
    "model = mobileunet(shape, num_classes)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(lr), metrics=['accuracy'])\n",
    "\n",
    "train_dataset = tf_dataset(img_train, mask_train, batch = batch_size)\n",
    "valid_dataset = tf_dataset(img_val, mask_val, batch = batch_size)\n",
    "\n",
    "train_steps = len(img_train)//batch_size\n",
    "valid_steps = len(img_val)//batch_size\n",
    "\n",
    "checkpointer = [\n",
    "    ModelCheckpoint(filepath=\"./model.h5\",monitor='val_loss',verbose=2,save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.1, verbose=2, min_lr=1e-6),\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=2)\n",
    "]\n",
    "\n",
    "model.fit(train_dataset,\n",
    "          steps_per_epoch=train_steps,\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=valid_steps,\n",
    "          epochs=epochs,\n",
    "          callbacks=checkpointer\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 320, 480, 3) for input Tensor(\"input_2_2:0\", shape=(None, 320, 480, 3), dtype=float32), but it was called on an input with incompatible shape (None, 768, 1152, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [01:14<00:00,  2.15it/s]\n"
     ]
    }
   ],
   "source": [
    "## Prediction\n",
    "H = 320   #to keep the original ratio \n",
    "W = 480 \n",
    "num_classes = 23\n",
    "\n",
    "create_dir('../results')  #create the folder for the predictions\n",
    "\n",
    "# Seeding\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Model\n",
    "model = tf.keras.models.load_model(\"model.h5\")\n",
    "\n",
    "# Saving the masks\n",
    "for x, y in tqdm(zip(img_test, mask_test), total=len(img_test)):\n",
    "    name = x.split(\"/\")[-1]\n",
    "    \n",
    "    ## Read image\n",
    "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "    x = x/255.0\n",
    "    x = x.astype(np.float32)\n",
    "\n",
    "    ## Read mask\n",
    "    y = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "    y = cv2.resize(y, (W, H))\n",
    "    \n",
    "    y = np.expand_dims(y, axis=-1) #(384,256,1)\n",
    "    \n",
    "    y = y * (255/num_classes)\n",
    "    y = y.astype(np.int32)\n",
    "    y = np.concatenate([y, y, y], axis=2)\n",
    "    \n",
    "    ## Prediction\n",
    "    p = model.predict(np.expand_dims(x, axis=0))[0]\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    \n",
    "    p = np.expand_dims(p, axis=-1)  \n",
    "    \n",
    "    p = p * (255/num_classes)\n",
    "    p = p.astype(np.int32)\n",
    "    p = np.concatenate([p, p, p], axis=2)\n",
    "      \n",
    "    cv2.imwrite(f\"../results/{name}\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160/160 [00:00<00:00, 264104.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# From the test set, take only images that represent the ones in the original dataset and not those are obtained from the data augmentation.\n",
    "# (they have _0 in the name)\n",
    "\n",
    "image_list = []\n",
    "mask_list = []\n",
    "\n",
    "for x,y in tqdm(zip(img_test, mask_test), total=len(img_test)):\n",
    "    name = x.split(\"/\")[-1]\n",
    "    image_name = name[4]\n",
    "\n",
    "    name = y.split(\"/\")[-1]\n",
    "    mask_name = name[4]\n",
    "    \n",
    "    if image_name == '0':\n",
    "        image_list.append(x)\n",
    "        mask_list.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot 5 images to verify the accuracy in the predictions\n",
    "\n",
    "img_selection = image_list[10:15]\n",
    "mask_selection = mask_list[10:15]\n",
    "\n",
    "for img, mask in zip(img_selection, mask_selection):\n",
    "    name = img.split(\"/\")[-1]\n",
    "    x = cv2.imread(img, cv2.IMREAD_COLOR)\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = cv2.resize(x, (W, H))\n",
    "\n",
    "    y = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n",
    "    y = cv2.resize(y, (W, H))\n",
    "\n",
    "\n",
    "    p = cv2.imread(f\"../results/{name}\", cv2.IMREAD_GRAYSCALE)\n",
    "    p = cv2.resize(p, (W, H))\n",
    "\n",
    "    #Plotto le tre immagini\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 20), constrained_layout=True)\n",
    "\n",
    "    axs[0].imshow(x, interpolation = 'nearest')\n",
    "    axs[0].set_title('image')\n",
    "    axs[0].grid(False)\n",
    "\n",
    "    axs[1].imshow(y, interpolation = 'nearest')\n",
    "    axs[1].set_title('GT')\n",
    "    axs[1].grid(False)\n",
    "\n",
    "    axs[2].imshow(p)\n",
    "    axs[2].set_title('prediction')\n",
    "    axs[2].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52f9c0ea18754e7cdedebb9c85b0bd3c4c8ba088ce0cbdb5d5f40d5264a2f94d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
